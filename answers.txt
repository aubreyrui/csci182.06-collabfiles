A. Why did you design the output of the network to be what you have in number 2? What was the idea as to why you chose this approach? (5 pts)

Our group decided to create a classification-based model that, given a set english input, outputs its highest probabilistic 
corresponding tagalog output. This makes it so that, instead of generating texts (this is what more complex translating models
do), it predicts texts to its corresponding tagalog label, making it viable for a simple english to tagalog direct translator
given the limited dataset we were only able to use. Following this, we set our epoch to only 30 and learning_rate to 0.0055 as the number
of epochs wont really change the maximum accuracy our model can produce (any epoch above 10 will do). As for learning_rate, we 
decided to keep 0.0055 as we find it the most consistent, but for higher accuracy predictions, increasing the epoch and decreasing
the learning rate may improve the model, but not to a significant degree (there was a case, but it somehow cannot be replicated, so
we assume it was just a lucky evaluation score). Batch size remained 32 for consistency.

As for optimizer, we opted to use Adam as it is the most popular and most effective optimizer for such task at hand.

B. What special tokens did you insert in your vocabulary to be able to have a better translation model? (5 pts)
For the vocabulary to work better with the tranlation model, we used the special token <unk> to handle out-of-vocabulary words.
<unk>, short for unknown, acts as a placeholder for words not found in the dataset during training. This added to the vocab with an index of 0.
That way, it ensures that the model can handle unknown words and still make predictions. However, the limitations would be that it treats all
unknown words the same, so the meaning of these words are overlooked.

C. Explain the loss function that you chose and why it is appropriate for your chosen model design (5 pts)
Our group chose the Cross Entropy loss function for this model because we wanted to measure how well the model was performing. 
The cross-entropy is a popular loss function that is used to measure the performance of a classification machine learning model. 
It measures the difference between the probability distribution of a classification model and the predicted values of the model. 
This function is also known as logarithmic loss or log loss. More specifically, we wanted to quantify the ability of our model 
to correctly match the actual translation outcomes we wanted to acquire. Our group also opted to use Cross Entropy loss in comparison 
to something like MSE (Mean Squared Error) loss as we were making a classification model, and not developing a regression model.


D. If you were to input an english statement, how would you measure its correctness of translation? (5 pts)
Measuring its correctness in translating the statement will be done within the translation model itself. 
The translation model is set into evaluation mode to ensure consistency. Within the model, the sentences in the statement are split 
into chunks and are translated separately. After that, the softmax function gets the probability of the translated sentence from 
the trained classification model being similar to the given dataset. Then, using torch.topk, it gets the top 3 predicted Tagalog 
sentence indices within the dataset and their corresponding probabilities. The highest probability, in terms of percentage, 
among the three will be the output and will be placed back in the statement. Therefore, consistently high percentages across all
sentences constitute the translated statement most likely correct.
